{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c795d7c6-bc3d-4ab9-9814-1cb2a1f4e270",
   "metadata": {},
   "source": [
    "# Read in the data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1de6f1d7-7263-43a5-84de-844036145885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re # regex\n",
    "import shutil\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "# from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "# from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_curve, confusion_matrix, auc, accuracy_score\n",
    "# from sklearn.naive_bayes import MultinomialNB\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from tensorflow.keras.callbacks import *\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "871378b9-3750-43fd-abe9-cca5ad53b9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_json('./data/Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a235bbd8-891c-4c3d-a79c-7be5bfde90cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   is_sarcastic  28619 non-null  int64 \n",
      " 1   headline      28619 non-null  object\n",
      " 2   article_link  28619 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 670.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_json('./data/Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9637e-d712-4025-9949-1529fc7d411f",
   "metadata": {},
   "source": [
    "# Basic data cleanup and column removal #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ef9dce9-65d1-4258-8a7d-ee04ffd7f770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop('article_link', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "058ad720-895e-423e-8e12-cea9f5cc33be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   is_sarcastic  28619 non-null  int64 \n",
      " 1   headline      28619 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 447.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96563c90-4bb4-4bd8-9296-e818579f4365",
   "metadata": {},
   "source": [
    "# Pre-processing #\n",
    "1. contractions\n",
    "2. stop words\n",
    "3. lowercase\n",
    "4. stemming\n",
    "5. tokenize\n",
    "\n",
    "Q: Do we need to do all of these?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e69bd6-043c-4dd9-a771-2ed32580d7f3",
   "metadata": {},
   "source": [
    "### Contractions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8ae8e1ed-057c-474b-94c0-53cb24c70155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(sentence):\n",
    "    sentence = re.sub(r\"won\\'t\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22a769f3-f684-4c53-aa5a-0e9ff6fc936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        former versace store clerk sues over secret 'b...\n",
       "1        the 'roseanne' revival catches up to our thorn...\n",
       "2        mom starting to fear son is web series closest...\n",
       "3        boehner just wants wife to listen, not come up...\n",
       "4        j.k. rowling wishes snape happy birthday in th...\n",
       "                               ...                        \n",
       "26704                 american politics in moral free-fall\n",
       "26705                             america is best 20 hikes\n",
       "26706                                reparations and obama\n",
       "26707    israeli ban targeting boycott supporters raise...\n",
       "26708                    gourmet gifts for the foodie 2014\n",
       "Name: headline, Length: 26709, dtype: object"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"headline\"].apply(remove_contractions)\n",
    "test_dataset[\"headline\"].apply(remove_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fdee3301-5e97-48c1-b339-2794a3f55ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_dataset[\"is_sarcastic\"]\n",
    "y_test = test_dataset[\"is_sarcastic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c37d231-f03d-4021-ad78-8fea7b469594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   is_sarcastic  28619 non-null  int64 \n",
      " 1   headline      28619 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 447.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515213b6-d8cb-4b46-82e3-2059403365c1",
   "metadata": {},
   "source": [
    "### Stop Words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "074f8cca-46d8-4868-8952-b895e20bd4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/azolyomi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43736612-da8d-488d-9b4d-e6a77664d775",
   "metadata": {},
   "source": [
    "#### One way of doing it: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "86c23f89-2d42-4dde-a405-0599b313c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thirtysomething scientists unveil doomsday clock of hair loss\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[\"headline\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c0bda-3796-4ab0-87ec-a1bae85bc5f0",
   "metadata": {},
   "source": [
    "## Tokenize ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2279b6db-b50c-46a3-a980-b8baa8e9a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28619, 25) (26709, 25) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30885"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(train_dataset[\"headline\"])\n",
    "\n",
    "encoded_train = t.texts_to_sequences(train_dataset[\"headline\"])\n",
    "encoded_test = t.texts_to_sequences(test_dataset[\"headline\"])\n",
    "\n",
    "max_length = 25\n",
    "\n",
    "padded_train = pad_sequences(encoded_train, \n",
    "    maxlen = max_length, \n",
    "    padding = \"post\", \n",
    "    truncating = \"post\")\n",
    "\n",
    "padded_test = pad_sequences(encoded_test, \n",
    "    maxlen = max_length, \n",
    "    padding = \"post\", \n",
    "    truncating = \"post\")\n",
    "\n",
    "print(padded_train.shape, padded_test.shape, type(padded_train))\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "be1457c0-7679-4299-a69c-70c25db3f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0ecd2388-e6ed-4a0b-bddd-5bf174a9c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = \"./glove/glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e4e16d2-1ee7-4c01-bb14-fbc1bdc2675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 24737 words (6147 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = vocab_size + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1bac4f9-4a7c-4367-a887-c2c8f30316fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor = \"val_accuracy\", \n",
    "                          patience = 7, \n",
    "                          verbose = 1,  \n",
    "                          restore_best_weights = True, \n",
    "                          mode = 'max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = \"val_accuracy\", \n",
    "                              factor = .4642,\n",
    "                              patience = 3,\n",
    "                              verbose = 1, \n",
    "                              min_delta = 0.001,\n",
    "                              mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d3b6804d-c354-458f-be77-efa202228784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 25)]              0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 25, 100)           3088700   \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 32)                17024     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 16)                528       \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 16)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,106,286\n",
      "Trainable params: 17,586\n",
      "Non-trainable params: 3,088,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape = (max_length, ), name = \"input\")\n",
    "\n",
    "embedding = Embedding(input_dim = vocab_size + 2, \n",
    "                      output_dim = 100, \n",
    "                      weights = [embedding_matrix], \n",
    "                      trainable = False)(input)\n",
    "\n",
    "lstm = LSTM(32)(embedding)\n",
    "flatten = Flatten()(lstm)\n",
    "\n",
    "dense = Dense(16, activation = None, \n",
    "              kernel_initializer = \"he_uniform\")(flatten)\n",
    "\n",
    "dropout = Dropout(.25)(dense)\n",
    "activation = Activation(\"relu\")(dropout)\n",
    "output = Dense(2, activation = \"softmax\", name = \"output\")(activation)\n",
    "model = Model(inputs = input, outputs = output)\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2f6d4233-efac-42da-8c6d-c0b55212a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 16:06:34.845567: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-24 16:06:34.970206: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4/895 [..............................] - ETA: 20s - loss: 0.6931 - accuracy: 0.5000  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 16:06:35.060897: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895/895 [==============================] - ETA: 0s - loss: 0.5825 - accuracy: 0.6862"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-24 16:06:50.434173: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-24 16:06:50.489862: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "895/895 [==============================] - 24s 26ms/step - loss: 0.5825 - accuracy: 0.6862 - val_loss: 0.4703 - val_accuracy: 0.7793\n",
      "Epoch 2/12\n",
      "895/895 [==============================] - 22s 25ms/step - loss: 0.4620 - accuracy: 0.7873 - val_loss: 0.4246 - val_accuracy: 0.8039\n",
      "Epoch 3/12\n",
      "895/895 [==============================] - 22s 25ms/step - loss: 0.4193 - accuracy: 0.8122 - val_loss: 0.3838 - val_accuracy: 0.8319\n",
      "Epoch 4/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3885 - accuracy: 0.8306 - val_loss: 0.3668 - val_accuracy: 0.8328\n",
      "Epoch 5/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3664 - accuracy: 0.8413 - val_loss: 0.3710 - val_accuracy: 0.8356\n",
      "Epoch 6/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3520 - accuracy: 0.8476 - val_loss: 0.3329 - val_accuracy: 0.8547\n",
      "Epoch 7/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3386 - accuracy: 0.8546 - val_loss: 0.3433 - val_accuracy: 0.8524\n",
      "Epoch 8/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3293 - accuracy: 0.8571 - val_loss: 0.3168 - val_accuracy: 0.8619\n",
      "Epoch 9/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3183 - accuracy: 0.8645 - val_loss: 0.2891 - val_accuracy: 0.8778\n",
      "Epoch 10/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3070 - accuracy: 0.8678 - val_loss: 0.2961 - val_accuracy: 0.8705\n",
      "Epoch 11/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.3002 - accuracy: 0.8724 - val_loss: 0.2738 - val_accuracy: 0.8840\n",
      "Epoch 12/12\n",
      "895/895 [==============================] - 23s 25ms/step - loss: 0.2927 - accuracy: 0.8763 - val_loss: 0.2682 - val_accuracy: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x171dbe0d0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train, y_train, \n",
    "        validation_data = (padded_test, y_test), \n",
    "        epochs = 12, \n",
    "        batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "48aaa788-a4d1-41b7-97fb-4c593ba4f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_singular(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    decontracted = remove_contractions(input_data)\n",
    "    sequenced = t.texts_to_sequences([input_data])\n",
    "    padded_sequenced = pad_sequences(\n",
    "        sequenced,\n",
    "        maxlen=max_length,\n",
    "        padding = \"post\", \n",
    "        truncating = \"post\")\n",
    "    return padded_sequenced\n",
    "\n",
    "def standardize_map(input_array):\n",
    "    return map(standardize_singular, input_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "badea3f5-da5b-4e46-9f3e-e4e35339c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm pretty sure that's sarcastic... 0.6942241\n",
      "I buy it! 0.01684583\n",
      "I'm pretty sure that's sarcastic... 0.98533034\n",
      "I'm pretty sure that's sarcastic... 0.9825166\n",
      "I buy it! 0.1546543\n",
      "I buy it! 0.010574713\n",
      "I'm pretty sure that's sarcastic... 0.81611\n",
      "I'm pretty sure that's sarcastic... 0.6766111\n",
      "I'm pretty sure that's sarcastic... 0.9448826\n",
      "I'm pretty sure that's sarcastic... 0.7626414\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Breakthrough Procedure Allows Surgeons To Transplant Pig Rib Directly Into Human Mouth\",\n",
    "    \"Jan. 6 Committee Seeks Interview With Kevin McCarthy\",\n",
    "    \"CDC Shortens COVID Isolation Guidelines to One Pump Up Song on Way to Work\",\n",
    "    \"Report: Snickers Basically Protein Bar\",\n",
    "    \"Crappy Music Has Helped Moron Through Hardest Times In His Pointless Life\",\n",
    "    \"Is Eminem Leaving The Music Industry?\",\n",
    "    \"Breaking News: Eminem Scared Out Of Music Industry By A Polar Bear\",\n",
    "    \"Chancellor Yang praised for bold indecision\",\n",
    "    \"Breaking News: UCSB shuts down permanently after Yang decides its too annoying to be a chancellor\",\n",
    "    \"Munger Hall described as \\\"Absolutely Stunning\\\" by UCSB Chancellor\",\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "standardized = standardize_map(examples)\n",
    "\n",
    "prediction = model.predict(standardized)\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    if (prediction[i][1] >= 0.6):\n",
    "        print(\"I'm pretty sure that's sarcastic...\", prediction[i][1])\n",
    "    else:\n",
    "        print(\"I buy it!\", prediction[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3d69e7a-e93f-4a6a-9bce-87102006fdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_2_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/fake_news_v1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./saved_model/fake_news_v1/assets\n",
      "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x2814373a0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p saved_model\n",
    "model.save('./saved_model/fake_news_v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117a54d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
