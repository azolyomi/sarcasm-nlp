{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de6f1d7-7263-43a5-84de-844036145885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0-rc0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re # regex\n",
    "import shutil\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, confusion_matrix, auc, accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import *\n",
    "from tensorflow.keras import Model, Input, Sequential\n",
    "from datetime import datetime\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795d7c6-bc3d-4ab9-9814-1cb2a1f4e270",
   "metadata": {},
   "source": [
    "# Read in the data #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "871378b9-3750-43fd-abe9-cca5ad53b9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_link</th>\n",
       "      <th>headline</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/versace-b...</td>\n",
       "      <td>former versace store clerk sues over secret 'b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/roseanne-...</td>\n",
       "      <td>the 'roseanne' revival catches up to our thorn...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://local.theonion.com/mom-starting-to-fea...</td>\n",
       "      <td>mom starting to fear son's web series closest ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://politics.theonion.com/boehner-just-wan...</td>\n",
       "      <td>boehner just wants wife to listen, not come up...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.huffingtonpost.com/entry/jk-rowlin...</td>\n",
       "      <td>j.k. rowling wishes snape happy birthday in th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        article_link  \\\n",
       "0  https://www.huffingtonpost.com/entry/versace-b...   \n",
       "1  https://www.huffingtonpost.com/entry/roseanne-...   \n",
       "2  https://local.theonion.com/mom-starting-to-fea...   \n",
       "3  https://politics.theonion.com/boehner-just-wan...   \n",
       "4  https://www.huffingtonpost.com/entry/jk-rowlin...   \n",
       "\n",
       "                                            headline  is_sarcastic  \n",
       "0  former versace store clerk sues over secret 'b...             0  \n",
       "1  the 'roseanne' revival catches up to our thorn...             0  \n",
       "2  mom starting to fear son's web series closest ...             1  \n",
       "3  boehner just wants wife to listen, not come up...             1  \n",
       "4  j.k. rowling wishes snape happy birthday in th...             0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = pd.read_json('./data/Sarcasm_Headlines_Dataset.json', lines=True)\n",
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a235bbd8-891c-4c3d-a79c-7be5bfde90cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   is_sarcastic  28619 non-null  int64 \n",
      " 1   headline      28619 non-null  object\n",
      " 2   article_link  28619 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 670.9+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset = pd.read_json('./data/Sarcasm_Headlines_Dataset_v2.json', lines=True)\n",
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b9637e-d712-4025-9949-1529fc7d411f",
   "metadata": {},
   "source": [
    "# Basic data cleanup and column removal #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ef9dce9-65d1-4258-8a7d-ee04ffd7f770",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.drop('article_link', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "058ad720-895e-423e-8e12-cea9f5cc33be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   is_sarcastic  28619 non-null  int64 \n",
      " 1   headline      28619 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 447.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96563c90-4bb4-4bd8-9296-e818579f4365",
   "metadata": {},
   "source": [
    "# Pre-processing #\n",
    "1. contractions\n",
    "2. stop words\n",
    "3. lowercase\n",
    "4. stemming\n",
    "5. tokenize\n",
    "\n",
    "Q: Do we need to do all of these?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e69bd6-043c-4dd9-a771-2ed32580d7f3",
   "metadata": {},
   "source": [
    "### Contractions ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ae8e1ed-057c-474b-94c0-53cb24c70155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(sentence):\n",
    "    sentence = re.sub(r\"won\\'t\", \"will not\", sentence)\n",
    "    sentence = re.sub(r\"can\\'t\", \"can not\", sentence)\n",
    "    sentence = re.sub(r\"n\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'re\", \" are\", sentence)\n",
    "    sentence = re.sub(r\"\\'s\", \" is\", sentence)\n",
    "    sentence = re.sub(r\"\\'d\", \" would\", sentence)\n",
    "    sentence = re.sub(r\"\\'ll\", \" will\", sentence)\n",
    "    sentence = re.sub(r\"\\'t\", \" not\", sentence)\n",
    "    sentence = re.sub(r\"\\'ve\", \" have\", sentence)\n",
    "    sentence = re.sub(r\"\\'m\", \" am\", sentence)\n",
    "    return sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22a769f3-f684-4c53-aa5a-0e9ff6fc936b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        former versace store clerk sues over secret 'b...\n",
       "1        the 'roseanne' revival catches up to our thorn...\n",
       "2        mom starting to fear son is web series closest...\n",
       "3        boehner just wants wife to listen, not come up...\n",
       "4        j.k. rowling wishes snape happy birthday in th...\n",
       "                               ...                        \n",
       "26704                 american politics in moral free-fall\n",
       "26705                             america is best 20 hikes\n",
       "26706                                reparations and obama\n",
       "26707    israeli ban targeting boycott supporters raise...\n",
       "26708                    gourmet gifts for the foodie 2014\n",
       "Name: headline, Length: 26709, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[\"headline\"].apply(remove_contractions)\n",
    "test_dataset[\"headline\"].apply(remove_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdee3301-5e97-48c1-b339-2794a3f55ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train_dataset[\"is_sarcastic\"]\n",
    "y_test = test_dataset[\"is_sarcastic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c37d231-f03d-4021-ad78-8fea7b469594",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 2 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   is_sarcastic  28619 non-null  int64 \n",
      " 1   headline      28619 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 447.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515213b6-d8cb-4b46-82e3-2059403365c1",
   "metadata": {},
   "source": [
    "### Stop Words ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "074f8cca-46d8-4868-8952-b895e20bd4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andraszolyomi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43736612-da8d-488d-9b4d-e6a77664d775",
   "metadata": {},
   "source": [
    "#### One way of doing it: ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e5d1f10-b234-4d02-aa9e-8a88a5a2b3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords = stopwords.words('english')\n",
    "# stemmer = SnowballStemmer('english')\n",
    "# tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# def preprocess_text(sentence, stem = False):\n",
    "#     text = []\n",
    "#     for word in sentence:\n",
    "        \n",
    "#         if word not in stopwords:\n",
    "            \n",
    "#             if stem:\n",
    "#                 text.append(stemmer.stem(word).lower())\n",
    "#             else:\n",
    "#                 text.append(word.lower())\n",
    "#     return tokenizer.tokenize(\" \".join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86c23f89-2d42-4dde-a405-0599b313c438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thirtysomething scientists unveil doomsday clock of hair loss\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[\"headline\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7c0bda-3796-4ab0-87ec-a1bae85bc5f0",
   "metadata": {},
   "source": [
    "## Tokenize ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2279b6db-b50c-46a3-a980-b8baa8e9a779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28619, 25) (26709, 25) <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "30885"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(train_dataset[\"headline\"])\n",
    "\n",
    "encoded_train = t.texts_to_sequences(train_dataset[\"headline\"])\n",
    "encoded_test = t.texts_to_sequences(test_dataset[\"headline\"])\n",
    "\n",
    "max_length = 25\n",
    "\n",
    "padded_train = pad_sequences(encoded_train, \n",
    "    maxlen = max_length, \n",
    "    padding = \"post\", \n",
    "    truncating = \"post\")\n",
    "\n",
    "padded_test = pad_sequences(encoded_test, \n",
    "    maxlen = max_length, \n",
    "    padding = \"post\", \n",
    "    truncating = \"post\")\n",
    "\n",
    "print(padded_train.shape, padded_test.shape, type(padded_train))\n",
    "\n",
    "vocab_size = len(t.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be1457c0-7679-4299-a69c-70c25db3f27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ecd2388-e6ed-4a0b-bddd-5bf174a9c144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "path_to_glove_file = \"./glove/glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e4e16d2-1ee7-4c01-bb14-fbc1bdc2675d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 24737 words (6147 misses)\n"
     ]
    }
   ],
   "source": [
    "num_tokens = vocab_size + 2\n",
    "embedding_dim = 100\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Words not found in embedding index will be all-zeros.\n",
    "        # This includes the representation for \"padding\" and \"OOV\"\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    else:\n",
    "        misses += 1\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1bac4f9-4a7c-4367-a887-c2c8f30316fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor = \"val_accuracy\", \n",
    "                          patience = 7, \n",
    "                          verbose = 1,  \n",
    "                          restore_best_weights = True, \n",
    "                          mode = 'max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = \"val_accuracy\", \n",
    "                              factor = .4642,\n",
    "                              patience = 3,\n",
    "                              verbose = 1, \n",
    "                              min_delta = 0.001,\n",
    "                              mode = 'max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b6804d-c354-458f-be77-efa202228784",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-13 15:31:31.850871: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input (InputLayer)          [(None, 25)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 25, 100)           3088700   \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 32)                17024     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                528       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16)                0         \n",
      "                                                                 \n",
      " output (Dense)              (None, 2)                 34        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,106,286\n",
      "Trainable params: 17,586\n",
      "Non-trainable params: 3,088,700\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape = (max_length, ), name = \"input\")\n",
    "\n",
    "embedding = Embedding(input_dim = vocab_size + 2, \n",
    "                      output_dim = 100, \n",
    "                      weights = [embedding_matrix], \n",
    "                      trainable = False)(input)\n",
    "\n",
    "lstm = LSTM(32)(embedding)\n",
    "flatten = Flatten()(lstm)\n",
    "\n",
    "dense = Dense(16, activation = None, \n",
    "              kernel_initializer = \"he_uniform\")(flatten)\n",
    "\n",
    "dropout = Dropout(.25)(dense)\n",
    "activation = Activation(\"relu\")(dropout)\n",
    "output = Dense(2, activation = \"softmax\", name = \"output\")(activation)\n",
    "model = Model(inputs = input, outputs = output)\n",
    "\n",
    "model.compile(optimizer = \"adam\", loss = \"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f6d4233-efac-42da-8c6d-c0b55212a477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "895/895 [==============================] - 14s 15ms/step - loss: 0.3242 - accuracy: 0.8633 - val_loss: 0.2775 - val_accuracy: 0.8846\n",
      "Epoch 2/30\n",
      "895/895 [==============================] - 14s 15ms/step - loss: 0.2964 - accuracy: 0.8767 - val_loss: 0.2712 - val_accuracy: 0.8912\n",
      "Epoch 3/30\n",
      "895/895 [==============================] - 20s 22ms/step - loss: 0.2747 - accuracy: 0.8886 - val_loss: 0.2359 - val_accuracy: 0.9044\n",
      "Epoch 4/30\n",
      "895/895 [==============================] - 18s 20ms/step - loss: 0.2551 - accuracy: 0.8957 - val_loss: 0.2252 - val_accuracy: 0.9111\n",
      "Epoch 5/30\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 0.2367 - accuracy: 0.9059 - val_loss: 0.2028 - val_accuracy: 0.9196\n",
      "Epoch 6/30\n",
      "895/895 [==============================] - 13s 15ms/step - loss: 0.2252 - accuracy: 0.9131 - val_loss: 0.2303 - val_accuracy: 0.9008\n",
      "Epoch 7/30\n",
      "895/895 [==============================] - 13s 15ms/step - loss: 0.2103 - accuracy: 0.9186 - val_loss: 0.1955 - val_accuracy: 0.9228\n",
      "Epoch 8/30\n",
      "895/895 [==============================] - 13s 14ms/step - loss: 0.1986 - accuracy: 0.9222 - val_loss: 0.1899 - val_accuracy: 0.9262\n",
      "Epoch 9/30\n",
      "895/895 [==============================] - 15s 17ms/step - loss: 0.1849 - accuracy: 0.9303 - val_loss: 0.1549 - val_accuracy: 0.9433\n",
      "Epoch 10/30\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 0.1720 - accuracy: 0.9354 - val_loss: 0.1495 - val_accuracy: 0.9454\n",
      "Epoch 11/30\n",
      "895/895 [==============================] - 19s 21ms/step - loss: 0.1610 - accuracy: 0.9391 - val_loss: 0.1319 - val_accuracy: 0.9512\n",
      "Epoch 12/30\n",
      "895/895 [==============================] - 16s 18ms/step - loss: 0.1539 - accuracy: 0.9426 - val_loss: 0.1110 - val_accuracy: 0.9621\n",
      "Epoch 13/30\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 0.1435 - accuracy: 0.9475 - val_loss: 0.1242 - val_accuracy: 0.9539\n",
      "Epoch 14/30\n",
      "895/895 [==============================] - 16s 18ms/step - loss: 0.1322 - accuracy: 0.9509 - val_loss: 0.1226 - val_accuracy: 0.9545\n",
      "Epoch 15/30\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 0.1255 - accuracy: 0.9548 - val_loss: 0.0929 - val_accuracy: 0.9711\n",
      "Epoch 16/30\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 0.1175 - accuracy: 0.9592 - val_loss: 0.0929 - val_accuracy: 0.9665\n",
      "Epoch 17/30\n",
      "895/895 [==============================] - 16s 18ms/step - loss: 0.1109 - accuracy: 0.9615 - val_loss: 0.0866 - val_accuracy: 0.9697\n",
      "Epoch 18/30\n",
      "895/895 [==============================] - 16s 18ms/step - loss: 0.1049 - accuracy: 0.9634 - val_loss: 0.0995 - val_accuracy: 0.9610\n",
      "Epoch 19/30\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 0.0975 - accuracy: 0.9671 - val_loss: 0.0781 - val_accuracy: 0.9739\n",
      "Epoch 20/30\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 0.0912 - accuracy: 0.9690 - val_loss: 0.0623 - val_accuracy: 0.9811\n",
      "Epoch 21/30\n",
      "895/895 [==============================] - 16s 18ms/step - loss: 0.0834 - accuracy: 0.9721 - val_loss: 0.0612 - val_accuracy: 0.9820\n",
      "Epoch 22/30\n",
      "895/895 [==============================] - 14s 15ms/step - loss: 0.0825 - accuracy: 0.9718 - val_loss: 0.0576 - val_accuracy: 0.9817\n",
      "Epoch 23/30\n",
      "895/895 [==============================] - 14s 15ms/step - loss: 0.0769 - accuracy: 0.9744 - val_loss: 0.0659 - val_accuracy: 0.9790\n",
      "Epoch 24/30\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 0.0729 - accuracy: 0.9759 - val_loss: 0.0623 - val_accuracy: 0.9805\n",
      "Epoch 25/30\n",
      "895/895 [==============================] - 15s 17ms/step - loss: 0.0703 - accuracy: 0.9771 - val_loss: 0.0748 - val_accuracy: 0.9727\n",
      "Epoch 26/30\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 0.0654 - accuracy: 0.9790 - val_loss: 0.0419 - val_accuracy: 0.9874\n",
      "Epoch 27/30\n",
      "895/895 [==============================] - 14s 16ms/step - loss: 0.0631 - accuracy: 0.9796 - val_loss: 0.0461 - val_accuracy: 0.9872\n",
      "Epoch 28/30\n",
      "895/895 [==============================] - 15s 17ms/step - loss: 0.0618 - accuracy: 0.9792 - val_loss: 0.0509 - val_accuracy: 0.9852\n",
      "Epoch 29/30\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 0.0557 - accuracy: 0.9829 - val_loss: 0.0482 - val_accuracy: 0.9844\n",
      "Epoch 30/30\n",
      "895/895 [==============================] - 15s 16ms/step - loss: 0.0574 - accuracy: 0.9825 - val_loss: 0.0560 - val_accuracy: 0.9815\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x15785e9b0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_train, y_train, \n",
    "        validation_data = (padded_test, y_test), \n",
    "        epochs = 30, \n",
    "        batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0685203-dfbf-4d3a-8b64-bea89a0617fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.44996068740875\n"
     ]
    }
   ],
   "source": [
    "y_pred_softmax = model.predict(padded_test)\n",
    "y_pred = []\n",
    "for i in range(len(y_pred_softmax)):\n",
    "    if  y_pred_softmax[i][1] >= 0.6:\n",
    "        y_pred.append(1)\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "        \n",
    "print(\"Accuracy:\", 100*accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "508d7aeb-efdb-4f86-8743-e60f329550fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "exported_model = tf.keras.Sequential([\n",
    "    model,\n",
    "    tf.keras.layers.Activation('sigmoid')\n",
    "])\n",
    "\n",
    "exported_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy, optimizer=\"adam\", metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48aaa788-a4d1-41b7-97fb-4c593ba4f583",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(input_data):\n",
    "    lowercase = tf.strings.lower(input_data)\n",
    "    decontracted = remove_contractions(input_data)\n",
    "    sequenced = t.texts_to_sequences([input_data])\n",
    "    padded_sequenced = pad_sequences(\n",
    "        sequenced,\n",
    "        maxlen=max_length,\n",
    "        padding = \"post\", \n",
    "        truncating = \"post\")\n",
    "    return padded_sequenced\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "badea3f5-da5b-4e46-9f3e-e4e35339c959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm pretty sure that's sarcastic...\n",
      "I buy it!\n",
      "I'm pretty sure that's sarcastic...\n",
      "I'm pretty sure that's sarcastic...\n",
      "I'm pretty sure that's sarcastic...\n"
     ]
    }
   ],
   "source": [
    "examples = [\n",
    "    \"Breakthrough Procedure Allows Surgeons To Transplant Pig Rib Directly Into Human Mouth\",\n",
    "    \"Jan. 6 Committee Seeks Interview With Kevin McCarthy\",\n",
    "    \"CDC Shortens COVID Isolation Guidelines to One Pump Up Song on Way to Work\",\n",
    "    \"Report: Snickers Basically Protein Bar\",\n",
    "    \"Crappy Music Has Helped Moron Through Hardest Times In His Pointless Life\",\n",
    "]\n",
    "\n",
    "\n",
    "standardized = map(standardize, examples)\n",
    "\n",
    "prediction = exported_model.predict(standardized)\n",
    "\n",
    "for i in range(len(prediction)):\n",
    "    if (prediction[i][1] >= 0.6):\n",
    "        print(\"I'm pretty sure that's sarcastic...\")\n",
    "    else:\n",
    "        print(\"I buy it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d69e7a-e93f-4a6a-9bce-87102006fdcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
